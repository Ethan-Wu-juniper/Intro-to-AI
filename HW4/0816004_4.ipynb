{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0816004_4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNf1ZO4_sJK2"
      },
      "source": [
        "# Assignment 4: CNN\n",
        "\n",
        "## Description\n",
        "\n",
        "Implement a Convolutional Neural Network (CNN) classifier to predict whether a given icon image is the real / fake. Where the fake images were generated by TAs with a neural network.\n",
        "\n",
        "- You are not required to use Colab in this assignment, but you have to **submit your source code**.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "- https://lab.djosix.com/icons.zip\n",
        "- 64x64 RGB jpg images\n",
        "\n",
        "\n",
        "```\n",
        "real/           (10000 images)\n",
        "    0000.jpg\n",
        "    0001.jpg\n",
        "    ...\n",
        "    9999.jpg\n",
        "fake/           (10000 images)\n",
        "    0000.jpg\n",
        "    0001.jpg\n",
        "    ...\n",
        "    9999.jpg\n",
        "unknown/        (5350 images, testing set)\n",
        "    0000.jpg\n",
        "    0001.jpg\n",
        "    ...\n",
        "    5349.jpg\n",
        "```\n",
        "\n",
        "- Training set\n",
        "  - 20000 icons in `real/` and `fake/`\n",
        "  - You should predict 1 for icons in `real/` and 0 for icons in `fake/`\n",
        "- Testing set:\n",
        "  - 5350 icons in `unknown/`\n",
        "  - Your score depends on the **accuracy** on this testing set,  \n",
        "    so the prediction of each icon in `unknown/` should be submitted (totally 5350 predictions, see below).\n",
        "\n",
        "\n",
        "## Submission\n",
        "\n",
        "Please upload **2 files** to E3. (`XXXXXXX` is your student ID)\n",
        "\n",
        "1. **`XXXXXXX_4_result.json`**  \n",
        "  This file contains your model prediction for the testing set.  \n",
        "  You must generate this file with the function called `save_predictions()`.\n",
        "2. **`XXXXXXX_4_source.zip`**  \n",
        "  Zip your source code into this archive.\n",
        "\n",
        "\n",
        "## Hints\n",
        "\n",
        "- **Deep Learning Libraries**: You can use any deep learning frameworks (PyTorch, TensorFlow, ...).\n",
        "- **How to implement**: There are many CNN examples for beginners on the internet, e.g. official websites of the above libraries, play with them and their model architectures to abtain high accuracy on testing set.\n",
        "- **GPU/TPU**: Colab provides free TPU/GPU for training speedup, please refer to [this page in `pytut.pdf` on E3](https://i.imgur.com/VsrUh7I.png).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j94Kc1dsLaR"
      },
      "source": [
        "### Include this in your code to generate result file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUgYOZfUMvxl"
      },
      "source": [
        "import json\n",
        "\n",
        "def save_predictions(student_id, predictions):\n",
        "  # Please use this function to generate 'XXXXXXX_4_result.json'\n",
        "  # `predictions` is a list of int (0 or 1; fake=0 and real=1)\n",
        "  # For example, `predictions[0]` is the prediction given \"unknown/0000.jpg\".\n",
        "  # it will be 1 if your model think it is real, else 0 (fake).\n",
        "\n",
        "  assert isinstance(student_id, str)\n",
        "  assert isinstance(predictions, list)\n",
        "  assert len(predictions) == 5350\n",
        "\n",
        "  for y in predictions:\n",
        "    assert y in (0, 1)\n",
        "\n",
        "  with open('{}_4_result.json'.format(student_id), 'w') as f:\n",
        "    json.dump(predictions, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvCRhWlTyh42"
      },
      "source": [
        "\n",
        "\n",
        "# code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u9qEkG0gVgH"
      },
      "source": [
        "parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUzcmGZVgVvH"
      },
      "source": [
        "#load dataset\n",
        "TRAIN_SET_RATIO = 0.7\n",
        "#model\n",
        "CLASS_NUM = 13\n",
        "#training\n",
        "LEARNING_RATE = 0.0001\n",
        "MOMENTUM = 0.75\n",
        "#main\n",
        "EPOCH_NUM = 30\n",
        "WORKER_NUM = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1WiSEcAgDrH"
      },
      "source": [
        "load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXiH4gwFfRJh"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Requirements of pretrained VGG\n",
        "def my_loader(path):\n",
        "  img = Image.open(path)\n",
        "  preprocess = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "  ])\n",
        "  return preprocess(img)\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, img_paths, labels=[], loader=my_loader):\n",
        "    self.img_paths = img_paths\n",
        "    self.labels = labels\n",
        "    self.loader = loader\n",
        "  def __getitem__(self, idx):\n",
        "    path = self.img_paths[idx]\n",
        "    img = self.loader(path)\n",
        "    if len(self.labels) == 0:\n",
        "      return img\n",
        "    label = self.labels[idx]\n",
        "    return img, label\n",
        "  def __len__(self):\n",
        "    return len(self.img_paths)\n",
        "\n",
        "def load_dataset():\n",
        "  if not os.path.exists('icons.zip'):\n",
        "    url = 'https://lab.djosix.com/icons.zip'\n",
        "    assert os.system(f'wget {url}') == 0\n",
        "    assert os.system('unzip icons.zip') == 0\n",
        "\n",
        "  # Both ./fake and ./real contain 10000 icons\n",
        "  total = 10000\n",
        "  train_set_size = int(total * TRAIN_SET_RATIO)\n",
        "  # ./unknown contains 5350 icons\n",
        "  test_set_size = 5350\n",
        "\n",
        "  train_img_paths = []\n",
        "  val_img_paths = []\n",
        "  train_labels = []\n",
        "  val_labels = []\n",
        "  for label, dir in enumerate(['./fake', './real']):\n",
        "    train_img_paths += [f'{dir}/{n:04}.jpg' for n in range(train_set_size)]\n",
        "    val_img_paths += [f'{dir}/{n:04}.jpg' for n in range(train_set_size, total)]\n",
        "    train_labels += [label] * train_set_size\n",
        "    val_labels += [label] * (total - train_set_size)\n",
        "\n",
        "  test_img_paths = [f'./unknown/{n:04}.jpg' for n in range(test_set_size)]\n",
        "\n",
        "  train_set = MyDataset(train_img_paths, train_labels)\n",
        "  val_set = MyDataset(val_img_paths, val_labels)\n",
        "  test_set = MyDataset(test_img_paths)\n",
        "\n",
        "  return train_set, val_set, test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue9tpTKdfSnn"
      },
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1GMHTKKyy0J"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "def set_param_requires_grad(model):\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = models.densenet161(pretrained=True)\n",
        "set_param_requires_grad(model)\n",
        "in_features = model.classifier.in_features\n",
        "model.classifier = nn.Linear(in_features, CLASS_NUM)\n",
        "input_size = 224\n",
        "\n",
        "# Froze feature extracting layers\n",
        "for param in model.features.parameters():\n",
        "  param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i55ZhsmZJgiE"
      },
      "source": [
        "diagram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzpyRUbjJg1c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw_diagram(loss_log, acc_log, epoch_num):\n",
        "  graph, (plot1, plot2) = plt.subplots(nrows=2)\n",
        "  plot1.set_title('Loss')\n",
        "  plot1.plot(loss_log['train'], label='training')\n",
        "  plot1.plot(loss_log['val'], label='validation')\n",
        "  plot1.legend()\n",
        "  plot2.set_title('Accuracy')\n",
        "  plot2.plot(acc_log['train'], label='training')\n",
        "  plot2.plot(acc_log['val'], label='validation')\n",
        "  plot2.legend()\n",
        "  graph.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Swg--bHgQue"
      },
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG3o8R5GgQ73"
      },
      "source": [
        "\"\"\"\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    for inputs, lables in train_loader:\n",
        "        inputs, lables = inputs.to(device), lables.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, lables)\n",
        "        preds = outputs.max(1).indices\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "def val(model, device, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, lables in val_loader:\n",
        "            inputs, lables = inputs.to(device), lables.to(device)\n",
        "            output = model(inputs)\n",
        "            val_loss += F.nll_loss(output, lables, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(lables.view_as(pred)).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print('\\nval set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        val_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "\"\"\"\n",
        "import copy\n",
        "\n",
        "def train_and_validate_model(model, dataloaders, criterion, optimizer, epoch_num=25):\n",
        "  best_weights = None\n",
        "  best_acc = 0.0\n",
        "  loss_log = { 'train': [], 'val': [] }\n",
        "  acc_log = { 'train': [], 'val': [] }\n",
        "  \n",
        "  for epoch in range(epoch_num):\n",
        "    print(f'Epoch: {epoch + 1}/{epoch_num}')\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      # For each batch of data\n",
        "      for inputs, labels in dataloaders[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Use with statement to temporarily enable gradient calculation\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          preds = outputs.max(1).indices\n",
        "\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "      epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "      epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "      if phase == 'val' and epoch_acc > best_acc:\n",
        "        best_weights = copy.deepcopy(model.state_dict())\n",
        "        best_acc = epoch_acc\n",
        "\n",
        "      loss_log[phase].append(epoch_loss)\n",
        "      acc_log[phase].append(epoch_acc)\n",
        "\n",
        "      print(f'[{phase}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "  # Load best model weights\n",
        "  model.load_state_dict(best_weights)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qny38eNhgbxW"
      },
      "source": [
        "main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbjb9aiEgdXX"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_set, val_set, test_set = load_dataset()\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_set, batch_size=8, shuffle=True, num_workers=WORKER_NUM),\n",
        "    'val': DataLoader(val_set, batch_size=8, shuffle=True, num_workers=WORKER_NUM),\n",
        "    'test': DataLoader(test_set, batch_size=8, shuffle=False, num_workers=WORKER_NUM)\n",
        "}\n",
        "params_to_update = []\n",
        "for name, param in model.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    params_to_update.append(param)\n",
        "optimizer = optim.SGD(params_to_update, lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "\n",
        "model = train_and_validate_model(model, dataloaders, criterion, optimizer, EPOCH_NUM)\n",
        "\n",
        "\n",
        "predictions = []\n",
        "for inputs in dataloaders['test']:\n",
        "  inputs = inputs.to(device)\n",
        "  outputs = model(inputs)\n",
        "  preds = outputs.max(1).indices.tolist()\n",
        "  predictions += preds\n",
        "\n",
        "save_predictions('0816004', predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}